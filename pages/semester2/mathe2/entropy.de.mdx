# Entropie

Formel: $H(X) = - \sum_{i=1}^{n} p(x_i) \cdot \log_2 p(x_i)$

Auf deutsch: $H(X)$ ist die Entropie von $X$ und wird berechnet
Dabei ist $p(x_i)$ die Wahrscheinlichkeit, dass $X$ den Wert $x_i$ annimmt.
Dies wird mit dem Logarithmus zur Basis 2 (Logarithmus Dualis) multipliziert und negiert.

Die Entropie ist ein Maß für die Unordnung oder den Informationsgehalt einer Zufallsvariablen.

import { Callout } from "nextra-theme-docs";

<Callout type="warning">
  In der Prfüng auf die Wahrscheinlichkeiten achten, und manchmal wird der
  Logarithmus zur Basis 2 nicht verwendet.
</Callout>

## Beispiel

### Entropie für ein Gacha Game

#### Generell

- $3 \text{ Sterne}$: $p(x_i) = \frac{944}{1000}$ (94.4%)
- $4 \text{ Sterne}$: $p(x_i) = \frac{50}{1000}$ (5%)
- $5 \text{ Sterne}$: $p(x_i) = \frac{6}{1000}$ (0.6%)

$$H(X) = - \left( \frac{944}{1000} \cdot \log_2 \frac{944}{1000} + \frac{50}{1000} \cdot \log_2 \frac{50}{1000} + \frac{6}{1000} \cdot \log_2 \frac{6}{1000} \right) = 0.206$$
